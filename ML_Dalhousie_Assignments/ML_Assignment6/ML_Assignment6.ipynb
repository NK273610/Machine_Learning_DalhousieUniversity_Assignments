{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.3\n",
    "no_actions=2\n",
    "no_states=5\n",
    "no_hidden=20\n",
    "discountFactor = 0.5\n",
    "e = 0.1\n",
    "num_episodes = 5000\n",
    "list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=tf.placeholder(tf.float32, shape=(1, no_states))\n",
    "weight_1 = tf.Variable(tf.random_uniform(shape=[no_states,no_hidden]))\n",
    "weight_2=tf.Variable(tf.random_uniform(shape=[no_hidden,no_actions]))\n",
    "bias_1=tf.Variable(tf.zeros([no_hidden]))\n",
    "bias_2=tf.Variable(tf.zeros([no_actions]))\n",
    "output_action= tf.placeholder(tf.float32, shape=(1, 2))\n",
    "\n",
    "hidden_layer=tf.add(tf.matmul(input_data,weight_1),bias_1)\n",
    "hidden_layer=tf.nn.sigmoid(hidden_layer)\n",
    "output=tf.add(tf.matmul(hidden_layer,weight_2),bias_2)\n",
    "\n",
    "predictedValues = tf.argmax(output,1)\n",
    "loss = 0.5*(output_action - output)**2\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tau(s,a):\n",
    "    if s == 0 or s==4:\n",
    "        return 0\n",
    "    if a==1:\n",
    "        return s+1\n",
    "    else:\n",
    "        return s-1\n",
    "\n",
    "def rho(s,a):\n",
    "    if s==1 and a==0:\n",
    "        return 1\n",
    "    if s==3 and a==1:\n",
    "        return 2\n",
    "    return 0\n",
    "\n",
    "def calc_policy(Q):\n",
    "    policy=np.zeros(5)\n",
    "    for s in range(0,5):\n",
    "        action_idx=np.argmax(Q[s,:])\n",
    "        policy[s]=2*action_idx-1\n",
    "        policy[0]=policy[4]=0\n",
    "    return policy.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q table is given as  [[0.  0. ]\n",
      " [1.  0.5]\n",
      " [0.5 1. ]\n",
      " [0.5 2. ]\n",
      " [0.  0. ]]\n",
      "Best policy is given as  [ 0 -1  1  1  0]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        for s,a in product(range(no_states),range(no_actions)):\n",
    "            # j=j+1\n",
    "            _, allQ = sess.run([predictedValues, output], feed_dict={input_data: np.identity(no_states)[s:s + 1]})\n",
    "            if np.random.rand() < e:\n",
    "                a = 1-a\n",
    "\n",
    "            sNew = tau(s, a)\n",
    "            r = rho(s, a)\n",
    "            Q1 = sess.run(output, feed_dict={input_data: np.identity(no_states)[sNew:sNew + 1]})\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0, a] = r + discountFactor * maxQ1\n",
    "            if i>500:\n",
    "                pass\n",
    "            sess.run([trainer], feed_dict={input_data: np.identity(5)[s:s + 1], output_action: targetQ})\n",
    "            s = sNew\n",
    "    for s in range(no_states):\n",
    "         list.append(sess.run(output,feed_dict={input_data:np.identity(5)[s:s+1]})[0])\n",
    "    Q=np.array(list)\n",
    "    print \"Q table is given as \",Q\n",
    "\n",
    "    print \"Best policy is given as \",calc_policy(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERENCES\n",
    "I have taken reference from the given site for implementing this code. \n",
    "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
